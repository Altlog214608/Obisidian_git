

[![Optimized XGBoost Model with Small Dataset for Predicting Relative ...](https://tse2.mm.bing.net/th/id/OIP.YuKnqMuI-ZD0eGwA-or4qAHaEs?pid=Api)](https://www.mdpi.com/1996-1944/15/15/5298)

---

## 🧠 머신러닝 & 딥러닝 모델 정리

### 1. XGBoost Regression

- **개요**: Gradient Boosting 기반의 회귀 모델로, 여러 개의 트리를 순차적으로 학습하며 잔차(residual)를 줄여나갑니다.
    
- **원리**: 처음 트리가 예측 → 잔차 계산 → 다음 트리가 잔차를 보정 → 반복 학습 → 최종 예측.
    
- **활용**: 수치 예측, 피처 중요도 분석 등에 널리 사용됩니다.
    

![oaicite:5](https:)  
💡 이미지 1 위: 여러 트리가 반복 학습되며 잔차를 보정하는 XGBoost 구조입니다. ([MDPI](https://www.mdpi.com/1996-1944/15/15/5298?utm_source=chatgpt.com "Optimized XGBoost Model with Small Dataset for Predicting Relative ..."))

---
![[Pasted image 20250731175857.jpg]]
### 2. MLP (Multi‑Layer Perceptron)

- **개요**: 입력층 → 은닉층(n개) → 출력층으로 구성된 전형적인 신경망입니다.
    
- **특징**: 모든 노드가 완전 연결(Fully Connected)되어 있으며, 비선형 활성화 함수로 복잡한 패턴을 학습할 수 있습니다.
    
- **활용**: 추천 시스템, 분류, 회귀 등 다양한 일반 머신러닝 문제에 사용됩니다.
    

![MLP 구조도](https:)  
💡 이미지 2: 입력층에서 은닉층, 출력층으로 이어지는 MLP 구조입니다.

---

### 3. VAE (Variational Autoencoder)

- **개요**: 인코더와 디코더로 구성된 확률 기반 생성 모델로, 잠재 공간(latent space)을 통해 데이터를 압축하고 재구성합니다.
    
- **특징**: 인코더는 입력을 평균(μ), 분산(σ)으로 매핑 → 잠재 벡터 생성(z = μ + σ·ε) → 디코더가 재구성을 수행.
    
- **활용**: 이미지/데이터 생성, 이상치 탐지, 노이즈 제거 등에 활용됩니다.
    

![VAE 구조도](https:)  
💡 이미지 3: VAE의 Encoder–Latent–Decoder 흐름을 그림으로 표현한 구조입니다. ([ResearchGate](https://www.researchgate.net/figure/Workflow-of-Bayesian-Optimization_fig6_358525717?utm_source=chatgpt.com "Workflow of Bayesian Optimization. | Download Scientific Diagram"))

---

### 4. GAN (Generative Adversarial Network)

- **개요**: 생성자(Generator)와 판별자(Discriminator)가 적대적으로 경쟁하며 학습하는 생성 모델입니다.
    
- **작동 방식**:
    
    - 생성자: 랜덤 노이즈 → 가짜 샘플 생성
        
    - 판별자: 진짜 vs 가짜 샘플 구분
        
    - 서로 경쟁하며 두 모델 모두 발전
        
- **활용**: 고품질 이미지 생성, 데이터 증강, 딥페이크 등.
    

![oaicite:23](https:)  
💡 이미지 4: 생성자와 판별자가 상호작용하며 학습하는 기본 GAN 구조입니다. ([GeeksforGeeks](https://www.geeksforgeeks.org/generative-adversarial-network-gan/?utm_source=chatgpt.com "Generative Adversarial Network (GAN) - GeeksforGeeks"))

---

### 5. Bayesian Optimization (베이지안 최적화)

- **개요**: 평가 비용이 큰 함수의 최적값을 효율적으로 찾는 확률 기반 탐색 기법입니다.
    
- **구성 요소**:
    
    - Surrogate model (주로 Gaussian Process)
        
    - Acquisition function (익스펙티드 임프루브먼트 등)
        
- **과정**: 초기 샘플 → surrogate 모델 학습 → acquisition으로 다음 후보 탐색 → 실제 평가 → 반복 → 최적 파라미터 탐색.
    
- **활용**: 머신러닝 하이퍼파라미터 튜닝, 실험비용이 높은 최적화 문제 등에 적합.
    

![oaicite:31](https:)  
💡 이미지 5: Gaussian Process 기반 surrogate 모델과 acquisition 함수 루프 구조입니다. ([ResearchGate](https://www.researchgate.net/figure/Schematic-diagram-of-bayesian-optimization-process_fig3_370138057?utm_source=chatgpt.com "Schematic diagram of bayesian optimization process | Download ..."))

---

## 📋 요약 비교표

|모델/기법|목적/분류|구조 요약|주요 활용|
|---|---|---|---|
|**XGBoost Regression**|지도학습 (회귀)|Boosted 트리 반복 학습|수치 예측, 피처 중요도 분석|
|**MLP**|지도학습 (분류/회귀)|완전 연결 다층 신경망|일반 예측 문제|
|**VAE**|생성 모델 (비지도)|Encoder–latent–Decoder 구조의 확률 모델|데이터 생성, 이상치 탐지|
|**GAN**|생성 모델 (비지도)|Generator vs Discriminator 적대 학습 구조|고품질 이미지 생성|
|**Bayesian Opt.**|최적화 알고리즘|Gaussian Process + Acquisition Function|하이퍼파라미터 최적화|

---

위 이미지와 설명은 직접 다운로드해 Obsidian Notes에서 붙여넣기 활용하실 수 있습니다.  
더 자세한 코드 예제나 응용 사례도 필요하시면 언제든 요청해주세요!